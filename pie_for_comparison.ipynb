{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b37c5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ec4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pie_extended.cli.utils import get_tagger, get_model, download\n",
    "from typing import List\n",
    "import os\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f938a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_download = False\n",
    "if do_download:\n",
    "    for dl in download(\"grc\"):\n",
    "        x = 1\n",
    "\n",
    "model_name = \"grc\"\n",
    "tagger = get_tagger(model_name, batch_size=256, device=\"cpu\", model_path=None)\n",
    "\n",
    "sentences: List[str] = [\"ἄνδρα μοι ἔννεπε, μοῦσα, πολύτροπον. \"]\n",
    "from pie_extended.models.grc.imports import get_iterator_and_processor\n",
    "for sentence_group in sentences:\n",
    "    iterator, processor = get_iterator_and_processor()\n",
    "    print(tagger.tag_str(sentence_group, iterator=iterator, processor=processor) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/PerseusDL/treebank_data.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_dir = './treebank_data/v2.1/Greek/texts'\n",
    "latin_dir = './treebank_data/v2.1/Latin/texts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db4d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_treebank_file(file_path):\n",
    "\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    indexed_gold_sentences = []\n",
    "    file_id1 = os.path.basename(file_path)\n",
    "    print(file_id1)\n",
    "    print(len(root.findall('.//sentence')))\n",
    "\n",
    "    for i, sentence in enumerate(root.findall('.//sentence')):\n",
    "        words = []\n",
    "        for word in sentence.findall('.//word'):\n",
    "            words.append({\n",
    "                'form': word.get('form'),\n",
    "                'lemma': word.get('lemma'),\n",
    "                'postag': word.get('postag'),\n",
    "                'relation': word.get('relation'),\n",
    "                'head': word.get('head')\n",
    "            })\n",
    "        indexed_gold_sentences.append((file_id1,i, words))\n",
    "    return indexed_gold_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gold_directory(directory, filter_text):\n",
    "    all_sentences = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if filter_text in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                sentences = parse_treebank_file(file_path)\n",
    "\n",
    "                for sentence in sentences:\n",
    "                    all_sentences.append(sentence)\n",
    "\n",
    "                print(f\"Processed {file_path}\")\n",
    "\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65acd7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_sentences=process_gold_directory(greek_dir, 'tlg0012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73deafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_treebank_file(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    file_id2 = os.path.basename(file_path)\n",
    "\n",
    "    sentences = []\n",
    "    for i, sentence in enumerate(root.findall('.//sentence')):\n",
    "        words = [word.get('form') for word in sentence.findall('.//word')]\n",
    "        sentence_text = ' '.join(words)\n",
    "        sentences.append((file_id2, i, sentence_text))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e366b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_pie(sentences, model_name):\n",
    "    tagger = get_tagger(model_name, batch_size=256, device=\"cpu\", model_path=None)\n",
    "    iterator, processor = get_iterator_and_processor()\n",
    "\n",
    "    analyzed_sentences = []\n",
    "    for file_id2, index, sentence in sentences:\n",
    "        analysis = tagger.tag_str(sentence, iterator=iterator, processor=processor)\n",
    "        analyzed_sentences.append((file_id2, index, analysis))\n",
    "    return analyzed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a749da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(directory, filter_text, model_name, nb_of_sentences):\n",
    "    all_sentences = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if filter_text in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                sentences = parse_treebank_file(file_path)\n",
    "                all_sentences.extend(sentences)\n",
    "                print(f\"Processed {file_path}\")\n",
    "    analyzed_sentences = analyze_with_pie(all_sentences[:nb_of_sentences], model_name)\n",
    "    return analyzed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_sentences = 100\n",
    "pie_sentences = process_directory(greek_dir, 'tlg0012', 'grc', nb_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27bd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(treebank_sentences, cltk_sentences):\n",
    "    for (file_id_tb, index_tb, sentence_tb), (file_id_cltk, index_cltk, sentence_cltk) in zip(treebank_sentences, cltk_sentences):\n",
    "        print(file_id_tb)\n",
    "        if file_id_tb == file_id_cltk and index_tb == index_cltk:\n",
    "            if file_id_tb==file_id_cltk and index_tb==index_cltk :\n",
    "                print(\"treebank : \"+str(sentence_tb))\n",
    "                print(\"cltk data : \"+str(sentence_cltk))\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53687d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_results(treebank_sentences, pie_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76510260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    if text is not None:\n",
    "        return unicodedata.normalize('NFC', text)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def compare_annotations(tb_sentence, pie_sentence):\n",
    "    error_details = {\n",
    "        'lemma_errors': [],\n",
    "        'pos_errors': []\n",
    "    }\n",
    "\n",
    "    file_id2, index, pie_analysis = pie_sentence\n",
    "\n",
    "    for tb_word, pie_word in zip(tb_sentence[2], pie_analysis):\n",
    "        tb_lemma_normalized = normalize_unicode(tb_word['lemma'])\n",
    "        pie_lemma_normalized = normalize_unicode(pie_word['lemma'])\n",
    "\n",
    "        if tb_lemma_normalized != pie_lemma_normalized:\n",
    "            error_details['lemma_errors'].append({\n",
    "                'word': tb_word['form'],\n",
    "                'tb_lemma': tb_word['lemma'],\n",
    "                'pie_lemma': pie_word['lemma']\n",
    "            })\n",
    "\n",
    "        tb_pos_normalized = normalize_unicode(tb_word['postag'])\n",
    "        pie_pos_normalized = normalize_unicode(pie_word['pos'])\n",
    "\n",
    "        if tb_pos_normalized != pie_pos_normalized:\n",
    "            error_details['pos_errors'].append({\n",
    "                'word': tb_word['form'],\n",
    "                'tb_pos': tb_word['postag'],\n",
    "                'pie_pos': pie_word['pos']\n",
    "            })\n",
    "\n",
    "    return error_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8990612",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errors = {\n",
    "    'lemma_errors': [],\n",
    "    'pos_errors': []\n",
    "}\n",
    "\n",
    "for tb_sentence, pie_tuple in zip(treebank_sentences, pie_sentences):\n",
    "    errors = compare_annotations(tb_sentence, pie_tuple)\n",
    "    all_errors['lemma_errors'].extend(errors['lemma_errors'])\n",
    "    all_errors['pos_errors'].extend(errors['pos_errors'])\n",
    "\n",
    "print(\"Lemma Errors:\", all_errors['lemma_errors'])\n",
    "print(\"POS Errors:\", all_errors['pos_errors'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd05edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Filtrer les erreurs POS en excluant 'g', 'd', et 'b'\n",
    "filtered_pos_errors = [error for error in all_errors['pos_errors']\n",
    "                       if error['tb_pos'] is not None and\n",
    "                          error['tb_pos'][0] not in ['g', 'd', 'b']]\n",
    "\n",
    "# Compter les erreurs en ne tenant compte que des erreurs filtrées\n",
    "pos_error_counts = Counter([error['tb_pos'][0] for error in filtered_pos_errors])\n",
    "\n",
    "print(pos_error_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1998e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos_groups = list(pos_error_counts.keys())\n",
    "errors = list(pos_error_counts.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(pos_groups, errors, color='skyblue')\n",
    "plt.xlabel('Groupes de POS Tags')\n",
    "plt.ylabel('Nombre d\\'erreurs')\n",
    "plt.title('Erreurs de POS Tagging par CLTK par Groupe de Tags')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c67007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "output_file = '/results/pie_results.csv'\n",
    "\n",
    "# Écriture dans le fichier CSV\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # En-têtes\n",
    "    writer.writerow(['Word', 'TB Lemma', 'Pie Lemma', 'TB POS', 'Pie POS'])\n",
    "\n",
    "    for error in all_errors['lemma_errors']:\n",
    "        writer.writerow([error['word'], error['tb_lemma'], error['pie_lemma'], '', ''])\n",
    "\n",
    "    for error in all_errors['pos_errors']:\n",
    "        writer.writerow([error['word'], '', '', error['tb_pos'], error['pie_pos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_greek(text):\n",
    "    if text is not None:\n",
    "        return unicodedata.normalize('NFC', text)\n",
    "    return None\n",
    "\n",
    "def compare_global_annotations(tb_sentence, pie_data):\n",
    "    annotations = []\n",
    "\n",
    "    for tb_word in tb_sentence[2]:\n",
    "        tb_text = normalize_greek(tb_word['form'])\n",
    "        tb_lemma = normalize_greek(tb_word.get('lemma'))\n",
    "        tb_pos_short = tb_word['postag'][0] if tb_word['postag'] else None\n",
    "\n",
    "        # Trouver le token correspondant dans pie_data\n",
    "        pie_token = next((word for word in pie_data if normalize_greek(word['form']) == tb_text), None)\n",
    "\n",
    "        if pie_token:\n",
    "            pie_lemma = normalize_greek(pie_token.get('lemma', ''))\n",
    "            pie_pos_short = pie_token.get('pos', '')[0] if pie_token.get('pos') else None\n",
    "\n",
    "            lemma_match = tb_lemma == pie_lemma\n",
    "            pos_match = tb_pos_short == pie_pos_short\n",
    "\n",
    "            annotations.append({\n",
    "                'word': tb_text,\n",
    "                'tb_lemma': tb_lemma,\n",
    "                'pie_lemma': pie_lemma,\n",
    "                'lemma_match': lemma_match,\n",
    "                'tb_pos_short': tb_pos_short,\n",
    "                'pie_pos_short': pie_pos_short,\n",
    "                'pos_match': pos_match\n",
    "            })\n",
    "\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_pie = './results/pie_results.csv'\n",
    "\n",
    "with open(output_file_pie, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Word', 'TB Lemma', 'Pie Lemma', 'Lemma Match', 'TB POS Short', 'Pie POS Short', 'POS Match'])\n",
    "\n",
    "    for tb_sentence, pie_tuple in zip(treebank_sentences, pie_sentences):\n",
    "        file_id_pie, index_pie, pie_data = pie_tuple\n",
    "        if file_id_pie == tb_sentence[0] and index_pie == tb_sentence[1]:\n",
    "            try:\n",
    "                annotations = compare_global_annotations(tb_sentence, pie_data)\n",
    "                for annotation in annotations:\n",
    "                    writer.writerow([annotation['word'], annotation['tb_lemma'], annotation['pie_lemma'],\n",
    "                                     annotation['lemma_match'], annotation['tb_pos_short'], annotation['pie_pos_short'],\n",
    "                                     annotation['pos_match']])\n",
    "            except ValueError as e:\n",
    "                print(f\"Erreur dans les données : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb32ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pie_env",
   "language": "python",
   "name": "pie_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
